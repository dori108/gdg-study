{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNOwbsFyQq5WV1xDT8KqKpO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dori108/gdg-study/blob/main/6%EC%9E%A5%EC%9D%84_%EA%B3%B5%EB%B6%80%ED%95%98%EB%A9%B0_%ED%97%B7%EA%B0%88%EB%A6%AC%EB%8A%94_%EA%B0%9C%EB%85%90%EB%93%A4_%EC%A0%95%EB%A6%AC%ED%95%98%EA%B8%B0\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ 1. 데이터셋에서 “원하는 한 쌍만” 사용할 수 있나?\n",
        "\n",
        "🔸 원칙적으로는 가능합니다.\n",
        "CycleGAN은 쌍이 필요 없는 방식이지만, 실험이나 테스트 목적이라면\n",
        "“내가 고른 말 1장 ↔ 얼룩말 1장”처럼 원하는 이미지 한 쌍만 사용해서도 훈련시킬 수 있어요.\n",
        "\n",
        "단, 주의할 점:\n",
        "데이터가 너무 적으면 학습이 안 됩니다. 모델은 다양한 패턴을 일반화하려 하기 때문이에요.\n",
        "\n",
        "예를 들어, 말 1장 ↔ 얼룩말 1장으로 훈련시키면 G가 그냥 그 하나만 외워버릴 수 있음 (overfitting).\n",
        "\n",
        "✅ 2. 인스턴스 정규화 (InstanceNorm2d)란?\n",
        "\n",
        "한 이미지 내에서 채널마다 평균과 표준 편차를 구해서 정규화하는 방식\n",
        "\n",
        "왜?\n",
        "\n",
        "이미지 스타일 변환에서는 배치간 분포의 차이보다 이미지 내부 스타일이 더 중요함\n",
        "\n",
        "* 배치 간 분포차이란?\n",
        "그냥....배치(얼만큼의 뭉텅이로 기준 잡고 계산 할 것이냐) 기준으로 분포를 계산하는 것을 말하는 듯\n",
        "->예를들어 4장을 동시에 평균, 분산을 계산한다던지\n",
        "\n",
        "BatchNorm은?\n",
        "여러 이미지(batch)의 통계(평균, 분산)를 공유해서 정규화\n",
        "\n",
        "---\n",
        "예시\n",
        "\n",
        "Batch size = 2\n",
        "이미지 A: 밝은 사진 (픽셀 평균 = 0.8)\n",
        "이미지 B: 어두운 사진 (픽셀 평균 = 0.2)\n",
        "\n",
        "→ 전체 평균 = 0.5, 표준편차 계산됨\n",
        "\n",
        "→ 둘 다 평균 0.5 기준으로 정규화됨\n",
        "\n",
        "---\n",
        "→ 예시에서 보면 A,B는 스타일이 완전 다른데 공통 기준 0.5로 정규화되는 구조인 것을 확인할 수 있음\n",
        "\n",
        "결과적으로 특징이 섞여 버리는 것\n",
        "\n",
        "따라서 배치 크기가 작을 경우 불안정하다고 하는거야\n",
        "\n",
        "InstanceNorm은?\n",
        "이미지 한 장마다, 그리고 채널별로 정규화\n",
        "\n",
        "---\n",
        "각 이미지 A, B를 개별적으로 정규화\n",
        "즉, 이미지 A는 자기 평균(0.8) 기준으로, B는 자기 평균(0.2) 기준으로 정규화\n",
        "\n",
        "- 스타일 변환은 이미지마다 다른 조명, 색감, 질감을 유지하거나 바꾸는 작업이기에 각 이미지의 특성을 유지할 수 있다는 특징이 있다\n",
        "\n",
        "---\n",
        "\n",
        "→ 스타일 전환에 더 유리함\n",
        "\n",
        "\n",
        "✅ 3. PatchGAN\n",
        "\n",
        "이미지를 쪼개서 그 이미지가 정말 진짜(기준)처럼 생겼는지 판단함\n",
        "\n",
        "디테일 강조, 계산 효율 등을 위해\n",
        "\n",
        "✅ 4. 옵티마이저 종류 및 Adam\n",
        "\n",
        "옵티마이저는 모델의 가중치를 업데이트하는 방식을 말한다\n",
        "\n",
        "| 이름           | 특징                              | 자주 쓰이는 곳               |\n",
        "| ------------ | ------------------------------- | ---------------------- |\n",
        "| **SGD**      | 가장 기본적인 경사하강법                   | 간단한 모델, 전통적            |\n",
        "| **Momentum** | 관성 개념을 추가해서 진동 줄임               | 빠른 수렴                  |\n",
        "| **Adam**     | **SGD + Momentum + RMSprop** 조합 | **딥러닝에서 기본값처럼 많이 사용됨** |\n",
        "| **RMSprop**  | 변화가 큰 가중치에 더 강하게 조정             | RNN 등                  |\n",
        "| **Adagrad**  | 학습률 자동 조절, 드물게 사용               | 희귀 데이터                 |\n",
        "| **AdamW**    | Adam + 가중치 감쇠                   | 최신 트렌드에서 더 안정적         |\n",
        "\n",
        "\n",
        "여기서 Adam에 더 포커싱을 해본다면,,,\n",
        "\n",
        "분산과 평균을 추적하고 각 파라미터에 적응적으로 학습률을 조정해주는 옵티마이저\n",
        "\n"
      ],
      "metadata": {
        "id": "x2O4gPel3mPM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***옵티마이저 개념***\n",
        "Stochastic Gradient Descent (SGD)\n",
        "\n",
        "손실함수의 기울기에 따라 한 걸음씩 이동한다는 의미\n",
        "\n",
        "경사가 자주 바뀌면 진동이 생기기 때문에 모멘텀이 정의될 수 있는데, 모멘텀은 이전 방향과 속도를 고려해서 덜 흔들리며 이동하게 돕는다.\n",
        "-> 문제점 보완을 위해 고안된 파라미터\n",
        "\n",
        "공이 경사를 내려올 때 관성처럼 움직이는 방식\n",
        "\n",
        "adaptive 방식\n",
        "모든 가중치에 동일한 학습률을 적용한다.\n",
        "가중치마다 학습률이 달라지면 당연히 정교함이 늘어남\n",
        "\n",
        "🔸 AdaGrad\n",
        "자주 업데이트되는 가중치들은 변화가 많다는 것이기에 학습률을 낮춤\n",
        "\n",
        "🔸 RMSprop\n",
        "최근 변화량에만 집중하여 가중 학습률 적용\n",
        "→ RNN이나 영상 입력 등에서 성능 좋음"
      ],
      "metadata": {
        "id": "R7bJBwEK6tQ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔸 AdaGrad — 자주 업데이트된 가중치의 학습률 낮추기\n",
        "\n",
        "자주 변화하는 파라미터의 학습률을 줄이고 덜 변화하는 파라미터의 학습률은 유지한다.\n",
        "\n",
        "\n",
        "θ\n",
        "t+1\n",
        "​\n",
        " =θ\n",
        "t\n",
        "​\n",
        " −\n",
        "G\n",
        "t\n",
        "​\n",
        " +ε\n",
        "​\n",
        "\n",
        "η\n",
        "​\n",
        " ⋅∇L(θ\n",
        "t\n",
        "​\n",
        " )\n",
        "\n",
        " 수식을 봤을 때 G t가 지금 분자에 와있는데 이것 자체는 그래디언트 제곱의 누적합이기 때문에 이게 커지면 판별자의 학습률이 줄어듦\n",
        "\n",
        " 자주 업데이트 되는 파라미터는 그래디언트 누적합을 키우게 되잖아 따라서 너무 누적되면 학습률이 0에 가까워질 것을 우려하는 것이다\n",
        "\n",
        "---\n",
        "\n",
        " 🔸 RMSprop — AdaGrad의 \"학습 멈춤\" 문제를 해결\n",
        "\n",
        " 모든 과거 그래디언트의 제곱 누적을 쓰면 위와 같이 가중치를 조절해줘야하는 문제가 생기기 때문에\n",
        "\"최근 변화\"만 기억하면서 평균내는 방식을 쓰자는 아이디어를 낸다.\n",
        "\n",
        "💡 핵심 차이점:\n",
        "AdaGrad: 모든 과거 변화의 누적 → 무조건 감소\n",
        "따라서 이 감소율을 줄이기 위한 여러 방법이 제안되는데..\n",
        "\n",
        "RMSprop: 최근 변화에 가중치 → 일정하게 유지 가능\n",
        "\n",
        "---\n",
        "모멘텀은 평균을 제어하는 파라미터\n",
        "미분 적분에서의 기울기 딱 그 의미네 지점이 아니고 최근 변화량의 평균을 따라서 이동하는 방식을 이용한다.\n",
        "\n",
        "---\n",
        "🔸 AdaGrad도 분산(variance)을 제어하는가?\n",
        "정확히는 \"분산이 아니라 스케일링 요인\"을 조절하는 방식이에요.** 학습률을 조절하는 것이 핵심목표**\n",
        "\n",
        "AdaGrad는 **gradient 제곱의 누적값(G_t)**을 기준으로 스케일을 나눔\n",
        "\n",
        "이는 분산처럼 적용되긴하는데 실제로는 그래디언트 누적 제곱이 핵심이다.\n",
        "\n"
      ],
      "metadata": {
        "id": "1CFPH2DkPUJr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "고급 옵티마이저 (AdamW, Lookahead 등)\n",
        "\n",
        "🔸 AdamW — \"정확한 가중치 감쇠(Weight Decay)\"\n",
        "Adam에서 L2 정규화를 쓰면 학습률이랑 섞여 이상하게 작동함\n",
        "\n",
        "Adam을 사용하는데 있어서 문제가 발생, 해결방안 고안\n",
        "\n",
        "→ Weight Decay와 Adam을 분리해서 적용\n",
        "\n",
        "💡 차이:\n",
        "Adam: 정규화를 학습률과 같이 적용\n",
        "\n",
        "AdamW: 가중치 감쇠를 별도로 적용 (W ← W - lr × λW)\n",
        "\n",
        "---\n",
        "스케일을 나눈다는 것은 각 파라미터(가중치)의 변화량을 개별적으로 조정한다는 의미이다.\n",
        "\n",
        "| 파라미터 | 누적 gradient $G_t$ | 학습률 스케일 |\n",
        "| ---- | ----------------- | ------- |\n",
        "| W1   | 100.0             | 작아짐     |\n",
        "| W2   | 1.0               | 유지됨     |\n",
        "| W3   | 0.01              | 거의 그대로  |\n",
        "\n",
        "이런 경우 W1은 3보다 현저히 학습률이 떨어지겠지\n",
        "이를 위해 학습률을 조정해서 적절히 스케일링 해줘야 한다.\n",
        "---\n",
        "\n",
        "📌 요즘 Transformer 계열(BERT, ViT 등) 대부분 AdamW 사용\n",
        "\n",
        "\n",
        "🔸 Lookahead — “여러 단계를 예측해서 더 안정적으로 학습”\n",
        "내부에 \"기본 옵티마이저 (예: Adam)\"를 감싸서,\n",
        "여러 번 업데이트한 뒤 중간값을 반영\n",
        "\n",
        "동작 방식:\n",
        "기본 옵티마이저가 k번 weight 업데이트\n",
        "\n",
        "Lookahead가 전체 방향 보고 평균 업데이트\n",
        "\n",
        "→ “미리 보고 판단하는 학습”이 됨\n",
        "\n",
        "📌 장점:\n",
        "\n",
        "더 안정된 수렴\n",
        "\n",
        "탐색 성능 향상\n",
        "\n",
        "| 옵티마이저     | 특징                  | 사용 예          |\n",
        "| --------- | ------------------- | ------------- |\n",
        "| LAMB      | 큰 배치 학습을 위한 Adam 변형 | BERT 대규모 학습   |\n",
        "| Lion      | 메모리 효율 + 성능 우수      | 구글 최신 트렌드     |\n",
        "| AdaBelief | gradient의 신뢰도까지 고려  | 분산 감소, 민감도 낮춤 |\n",
        "\n",
        "\n",
        "🎯 최종 요약 정리\n",
        "개념\t핵심 이해\n",
        "\n",
        "AdamW\t가중치 감쇠 정확히 분리 적용\n",
        "Lookahead\t여러 스텝 예측해서 학습 안정화"
      ],
      "metadata": {
        "id": "xXQ8PD86Rhij"
      }
    }
  ]
}